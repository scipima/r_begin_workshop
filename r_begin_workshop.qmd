---
title: "Data Analysis and Visualisation with R"
format:
  html:
    theme: united
    embed-resources: true
    code-fold: true
    code-summary: "Show the code"
    toc: true
    toc-depth: 3
    number-sections: true
    number-depth: 3
    css: custom_styles.css
editor_options: 
  chunk_output_type: console
---

## Intro & Learning objectives

* Get familiar with an IDE (Integrated Development Environment).
* Know how to set up a data research project.
* Visualise data: choose an appropriate visualisation for your data; understand `ggplot2`'s layered approach.
* Be able to read in, process, and store/output data.
* A few advanced topics: functions, conditional execution, joins, loops.

In this doc, all code chunks are folded.
Readers are encouraged to immediately look at the code for Sections 5 and 6.
From Sections 7 onwards, the reader can start to jot down some initial code before seeing the solution.

Let's **manage expectations**:

* I tailored this workshop for a 2 hours session.
Depending on the overall level, we might not cover all the content - *that's ok*, you still get this [document](https://github.com/scipima/r_begin_workshop) if you want to explore further. 
* I won't be able to enter into any detail about `R` itself, statistics, or visualisation - and *that's also ok*. 
* The main objective here really is to give you a flavour of a few of the things that can be currently achieved with the kinds of software and hardware we have at our disposal.

I *assume* that you are now staring at an IDE - be it RStudio or VS Code - as per the email that was shared with you. 
Please shout if that is not the case!


## Intro
For this workshop, we borrow heavily from [Data Visualization for Social Science](https://socviz.co/index.html#preface).
This very markdown is based off the `R` package accompanying the book. 
You can use it to take notes, write your code, and produce a good-looking, reproducible document that records the work you have done. 

At the very top of the file is a section of *metadata*, or information about what the file is and what it does.
The metadata is delimited by three dashes at the start and another three at the end.
You can/should change the title, author, and date to the values that suit you.
Keep the `output` line as it is for now, however.
Each line in the metadata has a structure.
First the *key* (`title`, `author`, etc.), then a colon, and then the *value* associated with the key.
It is very picky when it comes with indentations, characters used, etc., so be very careful if you start tweaking it.


### This is a Quarto File
[Markdown](http://rmarkdown.rstudio.com) is a simple formatting syntax for authoring HTML, PDF, and MS Word documents, and is also the basis for [Quarto](https://quarto.org/). 

When you click the **Render** button a document will be generated that includes both content as well as the output of any embedded `R` code chunks within the document. 
A *code chunk* is a specially delimited section of the file. 
You can add one by moving the cursor to a blank line choosing `Code` > `Insert Chunk` from the RStudio menu. 
When you do, an empty chunk will appear:
```{r}
```

Code chunks are delimited by three backticks.
The opening backticks also have a pair of braces and the letter `r`, to indicate what language the chunk is written in (this is a clue to the fact that you can have multiple programming language in the document, such as `Python`, or `Bash`, or `SQL`).
You write your code inside the code chunks.
Write your notes and other material around them.


## Know your editor
Please have a look at your screen.
Some of the most important things to learn now:

* Console
* Editor
* Environment

For this workshop, we rely on RStudio.
An alternative you may explore is [VS Code](https://code.visualstudio.com/).


## Before you begin to explore and analyse, set up your workspace
A number of functions and packages come already installed in what is generally referred to as `R base` (and you can actually see what's in it by typing `base::` in your console, and let the auto-complete suggest you the full list of functions ... ).
However, most of the things we'll do in this workshop need further `libraries`.
To install them, make sure you have an Internet connection. 
Then *manually* run the code in the chunk below. 
If you just render the document, this will be skipped - **however, if this is the first time you run this, make sure to manually run the code chunk below**. 
We do this because you only need to install these packages once, not every time you run this file. 
Either knit the chunk using the little green "play" arrow to the right of the chunk area, or copy and paste the text into the console window.
```{r}
#| eval: false
#| echo: false

# This code will not be evaluated automatically.
# Notice the eval = FALSE declaration in the options section of the code chunk

my_packages <- c(
  "bit", "devtools", "gapminder", "ggrepel", "here", "janitor", "scales",
  "tictoc", "tidyverse", "unvotes", "vroom"
)
install.packages(my_packages)
devtools::install_github("kjhealy/socviz")

# repo set-up -----------------------------------------------------------------#
# check if directory exists to dump processed files, figures, etc.
if ( !dir.exists( here::here("figures") ) ) {
  # if the directory does not exist, create it
  dir.create(here::here("figures"))
}
```


### Load Libraries
Once you installed your libraries, each time your run this document you **must** load them. 
If we do not load them, R will not be able to find the functions contained in these libraries. 
The `tidyverse` includes `ggplot2` (for data visualisation) and other libraries such as `dplyr` (for data manipulation).
We also load the `socviz` and `gapminder` libraries.
```{r}
#| include: false

## Load the libraries we will be using
library(tidyverse)
library(here)
library(gapminder)
library(ggrepel)
library(socviz)
```

Notice that here an option is set: `include=FALSE`. 
This tells `R` to run this code but not to include the output in the final document. 

When you click the **Render** button a document will be generated that includes both content as well as the output of any embedded `R` code chunks within the document.

A final note about the `here` [library](https://here.r-lib.org/).
That is a great help if you have properly set up your project as I just indicated. 
All paths will become relative, and you can forget about remembering where you are in your machine.
The essential thing is that it relies on the `.RProject` file that is created automatically for you when you create a `Project` with `RStudio`.


## Data Visualisation
### Few initial words
`R` has a consolidated role in data visualisation, well beyond the initial remit of statistics. 
A few examples:

* `R` at [FT](https://johnburnmurdoch.github.io/slides/r-ggplot/#/).
* `R` at [BBC](https://bbc.github.io/rcookbook/?ck_subscriber_id=2670369721&utm_source=convertkit&utm_medium=email&utm_campaign=What%E2%80%99s%20New%20in%20R:%20May%206,%202024%20-%2013817383).
* `R` in the *Best Practices for Data Visualisation* of the [Royal Statistical Society](https://royal-statistical-society.github.io/datavisguide/?ck_subscriber_id=2670369721&utm_source=convertkit&utm_medium=email&utm_campaign=What%E2%80%99s%20New%20in%20R:%20May%206,%202024%20-%2013817383). 
* Resources compiled by people from [Posit](https://themockup.blog/static/slides/intro-plot#1).


### Let's start with some actual code
Let's open up the `gapminder` dataset. 
What kind of variables are we dealing with?
```{r}
head(gapminder)
```

Technically, this data is in a *long* format, and you can immediately spot it by looking at the repeated values in the first 2 columns.
An example of a data in *wide* format is provided by [Eurostat](https://ec.europa.eu/eurostat/databrowser/view/sdg_08_10/default/table?lang=en).

We can explore the data in a more structured way, by checking what classes do we have as columns, and get some descriptive statistics.
```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
str(gapminder) # classes and first values
glimpse(gapminder) # very similar, tidyverse way
summary(gapminder) # main stats
```

Notice how we use functions in `R`: we call a function - in the chunk above, `str()`, `glimpse()`, `summary()` - and then include an argument between parentheses - in this case, the entire dataset.

Let's now start to analyse the data.
Say that we have an hypothesis regarding how life expectancy (`lifeExp`) changes in relations with GDP per capita (`gdpPercap`).
More precisely, we may think that as GDP per capita increases, so should life expectancy.
As these look like continuous variables, we could plot them in a [scatterplot](https://r-graph-gallery.com/).
Graphically, if we look back at our data, we should see dots amassing in lower-left and upper-right sections of our graph.

There are at least two ways to create a plot within `ggplot()`.
As you'll see, there are usually multiple ways to achieve the exact same output with these programming languages.

* We can **assign** - with this symbol `<-` - a `ggplot()` plot to an object called `p` (the name doesn't matter, we can call it `mike` if we want to), and then add (literally, using `+`) layers on top of it.
```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
p <- ggplot(
  data = gapminder,
  mapping = aes(
    x = gdpPercap,
    y = lifeExp
  )
)
p + geom_point() # and you can keep on adding to this, see below ...
```

* Another way of achieving the same thing is as follows, which is more in line with a `tidyverse` style which heavily rely on pipes (`|>`). 
```{r}
#| eval: false
#| code-fold: show

#------------------------------------------------------------------------------#
gapminder |> # pass the data along the pipe
  ggplot(aes(x = gdpPercap, y = lifeExp)) + # no need to call the data
  geom_point() # layered ggplot approach

# plot(gapminder$lifeExp ~ gapminder$gdpPercap) # built-in graphics
```

Let's unpack what happens in the code above:

* we grab the data by calling `gapminder`
* on this data, we apply a function, namely `ggplot()`
* calling a function on that data means we get the exact same output as the `p` object above.
Thus, we can add a layer on top of that ggplot object, that is the `geom_point`.

Let's go back to the graph now.
Sometimes, when we face a cloud like the one above, it is useful to plot a trendline on top, to understand the overall pattern. 
Again, this is a further example of the *layered* approach in `ggplot2`, wherein we keep on adding aesthetic layers to our plot until we reach the desired output.
```{r}
p + geom_point() +
  geom_smooth()
```

Looking at the graph itself, perhaps that is too wiggly.
Using `method='lm'` (linear model) as an argument to `geom_smooth()` includes a simple OLS regression.
This also should make you aware of the danger of extrapolating from predictions of inappropriate models.
```{r}
# another way to get the same result
ggplot(
  data = gapminder,
  mapping = aes(
    x = gdpPercap,
    y = lifeExp
  )
) +
  geom_point() +
  geom_smooth(method = "lm")
```

Data is quite bunched up against the left side. 
Gross Domestic Product per capita is not normally distributed across our country years. 
The x-axis scale would probably look better if it were transformed from a linear scale to a log scale. 
For this we can use a function called `scale_x_log10()`. 
As you might expect this function scales the x-axis of a plot to a log 10 basis. 
To use it we just add it to the plot:
```{r}
p <- ggplot(
  data = gapminder,
  mapping = aes(
    x = gdpPercap,
    y = lifeExp
  )
)
p + geom_point() +
  geom_smooth(method = "gam") +
  scale_x_log10()
```

The labels on the tick-marks can be controlled through the `scale_*` functions.
Please notice that this we are using another library to have our tick labels in the dollar format. 
```{r}
p + geom_point() +
  geom_smooth(method = "gam") +
  scale_x_log10(labels = scales::dollar)
```

The viz above already is way more informative than the ones before, but there is more we can extract from the underlying data. 
For instance, now it may make sense to re-introduce the `lm` method we used before. 

In passing, also notice the use of different parameters in the viz. 
The `se` option commands the *standard error*, and we can switch it off by imputing `se=FALSE`. 
The `alpha` regulates the transparency of the objects, from 0 to 1. 
Further, we can add info and make more it *publication ready*.

To achieve that, we can first import a series of custom fonts and adopt a harmonised style for all our future plots. 
```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
## Libraries -------------------------------------------------------------------
pacman::p_load(char = c("showtext", "sysfonts") )

#------------------------------------------------------------------------------#
## Graphic Parameters ----------------------------------------------------------
# Load custom font
sysfonts::font_add(
  family = "EuropeaNarrow",
  regular = here::here("data_reference", "ep_graphics", "fonts", "EuropeaNarrow-Regular.otf"),
  bold = here::here("data_reference", "ep_graphics", "fonts", "EuropeaNarrow-Bold.otf")
)
sysfonts::font_add(
  family = "EuropeaNarrowSemibold",
  regular = here::here("data_reference", "ep_graphics", "fonts", "EuropeaNarrow-Semibold.otf")
)
showtext::showtext_auto()  # Enable showtext for rendering


#------------------------------------------------------------------------------#
# Set default ggplot theme for all plots
ggplot2::theme_set(
  theme_minimal(base_family = "EuropeaNarrow") +
    theme(
      plot.title.position = "plot",
      plot.title = element_text(face = "bold"),
      plot.caption.position = "plot",
      plot.caption = element_text(hjust = 0, size = 20),
      strip.text = element_text(family = "EuropeaNarrowSemibold", size=25),
      panel.grid.minor = element_blank(),
      panel.spacing.x = unit(1.5, "lines"),
      axis.text.y = element_text(size = 30),
      axis.text.x = element_text(size = 30),
    )
)
```

We can then add informative labels, title, subtitle, and caption.
```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
# Plot
ggplot(
  data = gapminder,
  mapping = aes(
    x = gdpPercap,
    y = lifeExp
  )
) +
  geom_point(alpha = 0.3) +
  geom_smooth(color = "orange", se = FALSE, linewidth = 1, method = "lm") +
  scale_x_log10(labels = scales::dollar) +
  labs(
    x = "GDP per capita (in log scale)",
    y = "Life Expectancy in Years",
    title = "Economic Growth and Life Expectancy",
    subtitle = "Data points are country-years",
    caption = "Source: Gapminder."
  ) 
```

Another way to think about the underlying data is to remind ourselves about the **groups** we have, represented by the `continent` variable.
In the code below, we include colours as an aesthetic, and map it on continents.
Notice that, by including also a `colour` parameter in the `geom_smooth` function, we are overriding the `colour` mapping in the `ggplot` function.
This has important consequences, and is a further example of the **sequential** execution typical of many programming languages (left -> right, top -> bottom). 
```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
ggplot(
  data = gapminder,
  mapping = aes(
    x = gdpPercap,
    y = lifeExp,
    color = continent
  )
) +
  geom_point(alpha = 0.3) +
  # This overrides the colour aes above just for the smoothing
  geom_smooth(color = "orange", se = FALSE, linewidth = 1, method = "lm") +
  scale_x_log10(labels = scales::dollar)
```

Indeed, see what happens when we do not override it.
We can immediately appreciate that the slopes of our regression are radically different depending on the groups - consider this your introduction to the *dangers of pooling*. 
```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
ggplot(
  data = gapminder,
  mapping = aes(
    x = gdpPercap,
    y = lifeExp,
    color = continent
  )
) +
  geom_point(alpha = 0.3) +
  # No override here, 1 line per continent
  geom_smooth(method = "lm") +
  scale_x_log10(labels = scales::dollar)
```

However, this has become too cluttered, and it's hard to understand what's going on with all these colours and lines.
We reached saturation and need to revert to a simpler plot. 
```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
ggplot(
  data = gapminder,
  mapping = aes(
    x = gdpPercap,
    y = lifeExp
  )
) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", colour = "orange", fill = "grey90") +
  facet_wrap(~continent) +
  scale_x_log10(labels = scales::dollar) +
  labs(
    x = "GDP per capita",
    y = "Life Expectancy in Years",
    title = "Economic Growth and Life Expectancy",
    subtitle = "Data points are country-years",
    caption = "Source: Gapminder."
  )
```

What you observe above is variously termed as a *small multiple* visualisation, or *faceted* visualisation. 
The key point is that we break down the data in meaningful groups, and analyse them separately.
This enables us to see more clearly similarities and differences.

A radically different take is to exploit the **time dimension**.
```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
gapminder |>
  # subset the data to just the rows with these continents
  filter(continent %in% c("Asia", "Africa", "Europe", "Americas")) |>
  ggplot(aes(x = year, y = gdpPercap)) +
  # 1 smoothing line per continent
  geom_line(color = "gray80", aes(group = country)) +
  geom_smooth(linewidth = 1, method = "loess", se = FALSE) +
  scale_y_log10(labels = scales::dollar) +
  facet_wrap(~continent, ncol = 2) +
  labs(
    x = "Year",
    y = "GDP per capita",
    title = "GDP per capita on five continents (1952-2007)"
  )
```

A few takeaways from this.

* First, our visualisation obviously brings the reader's attention to the blue trend line by continent.
For instance, the value at the end of the time series for the grouped trend line in Asia is roughly where the start of the grouped time series is in Europe.
* Second, pay attention to the larger variation surrounding the trend line in continents such as Asia and Africa compared to the relatively tighter distribution in Europe.
Inter alia, this would have substantial consequences were we to fit a model to such data: the larger variance in the former continents would mean larger uncertainty in our estimates compared to the European case.
In another case that you may be familiar with, this could happen to political parties' polls, where we could have substantial variation in the polls translating into larger margin of errors in projections.
```{r}
rm(p) # remove obsolete object
```


## UN Votes
Also in this Section the data comes directly from the library, exactly as in the case of `gapminder`.
How many tables are we dealing with?
```{r}
#| warning: false
#| message: false

library(unvotes)
un_votes <- unvotes::un_votes
un_roll_calls <- unvotes::un_roll_calls
```


### Counting categorical variables
If we want to get a quick glimpse of the distribution of votes, we could just simply count how many there are by unique value of the `vote` variable.

::: {.panel-tabset}

#### Base R

```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
with(
  un_votes,
  tapply(X = vote, INDEX = vote, FUN = length)
) |> sort(decreasing = TRUE)
```

#### R tidyverse

```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
un_votes |>
  count(vote, sort = TRUE)
```

#### R data.table

```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
data.table::setDT(un_votes)[, .N, by = list(vote)][order(-N)]
```

:::


Say that we don't particularly like this representation of the data, and we would like to recode the vote variable.
We could achieve that like so. 
```{r}
# recode
un_votes$vote_int[un_votes$vote == "yes"] <- 1L
un_votes$vote_int[un_votes$vote == "no"] <- -1L
un_votes$vote_int[un_votes$vote == "abstain"] <- 0L
# check
table(un_votes$vote_int, un_votes$vote, exclude = NULL)
```


### Aggregations
Say that now you are interested in the share of votes by country.
Check how you can do it in 2 ways, with a rough appreciation of the timing difference.
I'll walk you through just the `tidyverse` implementation.

* we call the data
* we group by a variable of interest
* we squash all the data by the grouping variable, and calculate two new variables, namely the total votes, and the number of yes.
* we close the grouped calculations by calling off the `group_by` - that is, `ungroup()`
* we create a new variable, which is just the share


::: {.panel-tabset}

#### R tidyverse

```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
library(tictoc) # library for timing

# aggregation: tidyverse style ------------------------------------------------#
tictoc::tic() # timing on

un_votes |>
  group_by(country) |> # group by country
  summarise(
    n_votes = n(), # count votes
    n_yes = sum(vote == "yes", na.rm = TRUE) # count yes
  ) |>
  ungroup() |> # ungroup operations
  mutate(pct_yes = n_yes / n_votes) # create new col, no summarise

tictoc::toc() # timing off
```

#### R data.table

```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
# aggregation: data.table style -----------------------------------------------#
tictoc::tic()

data.table::setDT(un_votes)[, list(
  n_votes = .N,
  n_yes = sum(vote == "yes", na.rm = TRUE)
),
keyby = list(country)
][, pct_yes := n_yes / n_votes][]

tictoc::toc()
```

:::


### Functions
As a more *advanced topic*, if you know you'll execute that same function over and over, you can re-package that in a `function`.
Notice a few things:

* name/position of arguments
* default values
* calling functions explicitly from `namespace`

```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
# write your first function
summarise_votes <- function(data_in = un_votes, min_tot_votes = 10L) {
  data_in |>
    dplyr::summarise(
      n_votes = n(),
      n_yes = sum(vote == "yes", na.rm = TRUE)
    ) |>
    dplyr::filter(n_votes >= min_tot_votes) |> # filter by at least this many votes
    dplyr::mutate(pct_yes = n_yes / n_votes) |>
    dplyr::arrange(dplyr::desc(pct_yes)) # arrange by pct_yes
}

# test, we should get the same answer as above ... -------------------------------#
un_votes |>
  group_by(country) |>
  summarise_votes() |> # use the newly created function
  ungroup()
```


### Trends over time in `yes` votes
Similar to the previous case - the `gapminder` data - we may be interested in the evolution over time. 
In order to get that piece of info, we need to introduce another topic, namely **merges** (or **joins**).
Indeed, votes' dates are recorded in the other dataset, `un_roll_calls`.
Notice the use of `left_join`.
```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
un_votes <- un_votes |> # start with this data
  dplyr::left_join( # join only the matches in the origin data
    y = un_roll_calls |> # from this data, we keep only the matches in the incoming data from the pipe
      dplyr::select(rcid, date), # we also subset the right-hand side data on the fly
    by = "rcid" # we merge by this col
  ) |>
  dplyr::mutate(year = lubridate::year(date)) # create a year col

# same result, this time in base R
# merge(un_votes, un_roll_calls,
#       by = "rcid",
#       all.x = TRUE, all.y = FALSE) # left join
```

Graph the evolution over time of share of yes.
```{r}
un_votes |>
  dplyr::group_by(year) |>
  summarise_votes() |>
  ungroup() |>
  ggplot(aes(x = year, y = pct_yes)) +
  geom_line() +
  expand_limits(y = 0) 
```

Again, beware of pooling.
What if we split the overall trend by country?
```{r}
un_votes |>
  dplyr::filter(country %in% c("France", "Germany", "Sweden", "United States")) |>
  dplyr::group_by(country, year) |> # notice we're grouping by 2 cols here
  summarise_votes() |>
  ungroup() |>
  ggplot(aes(x = year, y = pct_yes, group = country)) +
  geom_line(aes(colour = country)) +
  scale_y_continuous(labels = scales::label_percent()) +
  expand_limits(y = 0) +
  theme(legend.position = "top")
```

This can become too cluttered very soon.
How do we deal with that?
Faceting is your friend.
However, depending on the country, the plot may be similar or way off from the aggregate.
So, we can remind ourselves in every graph what the overall trend is.
Now, you can tweak your code to represent your overall trend in any way you like (e.g. regional data?). 
```{r}
#| warning: false
#| message: false
#| fig-width: 8
#| fig-height: 7

#------------------------------------------------------------------------------#
# Plot
un_votes |>
  dplyr::filter(
    country %in% c(
      "France", "Germany", "Sweden", "United States", "Russia", "China"
    )
  ) |>
  dplyr::group_by(country, year) |> # notice we're grouping by 2 cols here
  summarise_votes() |>
  ungroup() |>
  ggplot() + # notice this is empty
  geom_line(aes(
    x = year, y = pct_yes, # we're passing aes directly to geoms
    group = country, colour = country
  ), 
  show.legend = FALSE) +
  # here we're providing new data, aggregated on the fly
  geom_line(
    data = un_votes |>
      dplyr::group_by(year) |>
      summarise_votes() |>
      ungroup(),
    aes(x = year, y = pct_yes),
    colour = "grey", linewidth = 1
  ) +
  facet_wrap(~country, ncol = 3) +
  scale_y_continuous(labels = scales::label_percent()) +
  labs(x = NULL, y = "Share of yes (%)") +
  expand_limits(y = 0) +
  theme(
    axis.title.y = element_text(size = 30)
  )
# Store the figure locally
ggsave(here::here("figures", "unvotes_yesbycountry.png"), 
       dpi = 300, height = 6, width = 8, device = "png", bg = "white")
```

```{r}
#| include: false
rm(un_roll_calls, un_votes, summarise_votes)
gc()
```


## EP Data
### How many documents in each Committee?
Say that you want to organise the EP so that the Committees that receive the most files are also allocated more manpower.
How would you go about answering this question?

* Find the data.
* Load it in your machine.
* Process it.
* Spit back the result.

```{r}
#| code-fold: show
#| warning: false
#| message: false

#------------------------------------------------------------------------------#
# Download and Process EP Open Data on Plenary Docs ----------------------------
#------------------------------------------------------------------------------#

#' Collects .csv files from the [EP Open Data Portal](https://data.europarl.europa.eu/en/datasets?language=en&order=RELEVANCE&dataThemeFamily=dataset.theme.EP_PLEN_DOC).
#' The code then proceeds to tidy such data and write to disk.

## Texts tabled: read csv & append it all together -----------------------------
docs_csv <- c(
  "https://data.europarl.europa.eu/distribution/plenary-documents_2024_15_en.csv",
  "https://data.europarl.europa.eu/distribution/plenary-documents_2023_52_en.csv",
  "https://data.europarl.europa.eu/distribution/plenary-documents_2022_33_en.csv",
  "https://data.europarl.europa.eu/distribution/plenary-documents_2021_22_en.csv",
  "https://data.europarl.europa.eu/distribution/plenary-documents_2020_14_en.csv",
  "https://data.europarl.europa.eu/distribution/plenary-documents_2019_5_en.csv"
)
# read all .csv at once
docs_list <- lapply(X = docs_csv, readr::read_csv)

#------------------------------------------------------------------------------#
# Same thing, as a loop -------------------------------------------------------#
# docs_list <- NULL
# for (i_doc in docs_csv) {
#   print(i_doc)
#   docs_list[[i_doc]] <- read.csv(file = i_doc) }
#------------------------------------------------------------------------------#

# append all .csv together ----------------------------------------------------#
plenary_docs <- data.table::rbindlist(l = docs_list, use.names = TRUE, fill = TRUE)
```

```
#------------------------------------------------------------------------------#
rm(docs_list, docs_csv)
gc()
```

Extract the year feature, similar to what we did above, so that we can group the data afterwards.  
```{r}
# extract year
plenary_docs$year <- lubridate::year(plenary_docs$document_date)
# unique(plenary_docs$year) # check
```

Do we have duplicate entries in the doc id?
We can table the number of times each document appears in the file.
```{r}
table(plenary_docs$document_identifier, exclude = NULL) |>
  head(5)
```

We can then take the mean of the cross-tabulation above.
If the mean is above 1, then we have multiple entries.
```{r}
mean(table(plenary_docs$document_identifier, exclude = NULL))
```

Say that, for whtever reason, you know you cannot have more than 1 DOC ID.
If the data shows you a count higher than 1, then you prefer to stop altogether and issue an informative error message.
You can do the following.
```{r}
# Example of control flow execution
if ( mean(table(plenary_docs$document_identifier, exclude = NULL)) > 1 ) {
  # Isolate duplicates
  duplicates = plenary_docs$document_identifier[
    table(plenary_docs$document_identifier, exclude = NULL) > 1
  ]
  stop(paste0("\nThe DOC ID does not appear to be unique. Check your data\n",
              paste0(duplicates, collapse = "; "))
  )
} else {
 cat("\nYou have no duplicates in your data. Good to go.\n") 
}
```

We can then extract the unique names of the Committees in the text.
```{r}
head(sort(unique(plenary_docs$document_creator_organization)))
# tail(sort(unique(plenary_docs$document_creator_organization))) # inspect the last items
```

There are way too many values here.
We need to filter on the string to get just the pattern of interest, namely `Committee on `.
```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
plenary_docs |>
  dplyr::filter(
    grepl( # REGEX
      pattern = "Committee on ",
      x = document_creator_organization
    )
  ) |>
  dplyr::group_by(document_creator_organization) |>
  dplyr::summarise(count = n()) |>
  dplyr::arrange(dplyr::desc(count))
```

However, we're not done here.
If we actually check the tail of the table above - by simply running `tail()` -, we can notice something weird. 
Some Committees appear to have multiple DOC IDs associated with them - these are *Joint Committees*.
```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
plenary_docs |>
  dplyr::filter(
    grepl(
      pattern = "Committee on ",
      x = document_creator_organization
    )
  ) |>
  tidyr::separate_longer_delim(cols = document_creator_organization, delim = ";") |>
  dplyr::mutate(
    document_creator_organization = trimws(document_creator_organization)
  ) |>
  dplyr::select(document_identifier, document_creator_organization) %>%
  dplyr::group_by(document_identifier) |>
  dplyr::summarise(committee_count = n()) |>
  dplyr::ungroup() |>
  dplyr::arrange(dplyr::desc(committee_count))
```

We need to split the cells that contain multiple Committees.
```{r}
#| code-fold: show

#------------------------------------------------------------------------------#
comm_doc_count <- plenary_docs |>
  dplyr::filter(
    grepl(
      pattern = "Committee on ",
      x = document_creator_organization
    )
  ) |>
  tidyr::separate_longer_delim(
    cols = document_creator_organization, # split this col
    delim = ";" # on this delimiter
    ) |>
  dplyr::mutate(
    document_creator_organization = trimws(document_creator_organization)
  ) |>
  dplyr::group_by(document_creator_organization) |>
  dplyr::summarise(doc_count = n()) |>
  dplyr::ungroup() |>
  dplyr::arrange(dplyr::desc(doc_count))
```

And we can plot to visually inspect that everything went as intended.
```{r}
comm_doc_count |>
  dplyr::filter(doc_count > 1L) |>
  dplyr::mutate(document_creator_organization = forcats::fct_reorder(
    .f = document_creator_organization, .x = doc_count, .fun = max
  )) |>
  ggplot(aes(x = document_creator_organization, y = doc_count)) +
  geom_col(colour = "black", fill = "grey", linewidth = 0.1) +
  coord_flip() +
  labs(x = "", y = "Count") 
```

Now, before seeing the code below, try to come up with your solution: what if you wanted also to know the volumes per Committee per year?
```{r}
#| fig-width: 8
#| fig-height: 10
#| warning: false
#| message: false

#------------------------------------------------------------------------------#
plenary_docs |>
  dplyr::filter(grepl(
    pattern = "Committee on ",
    x = document_creator_organization
  )) |>
  tidyr::separate_longer_delim(cols = document_creator_organization, delim = ";") |>
  dplyr::mutate(
    document_creator_organization = trimws(
      gsub(
        pattern = "Committee on |Committee on the ", replacement = "",
        x = document_creator_organization
      )
    )
  ) |>
  dplyr::group_by(document_creator_organization, year) |>
  dplyr::summarise(doc_count = n()) |>
  dplyr::ungroup() |>
  dplyr::arrange(dplyr::desc(doc_count)) |>
  dplyr::filter(doc_count > 1L) |>
  dplyr::mutate(year_date = as.Date(paste0(year, "-01-01"))) |>
  ggplot(aes(x = year_date, y = doc_count)) +
  geom_line(colour = "black", linewidth = 1) +
  facet_wrap(~document_creator_organization,
             ncol = 4,
             labeller = labeller(document_creator_organization = label_wrap_gen(20))
  ) +
  labs(x = "", y = "Count") +
  theme(strip.text = element_text(lineheight = 0.5)) 
```

```{r}
#| include: false

#------------------------------------------------------------------------------#
rm(plenary_docs); gc()
```


## Polls
This section shows you how you can download, process, and plot polls data relative to one country.
Polls are extracted from [EuropeElects](https://europeelects.eu/germany/).
```{r}
#| fig-width: 8
#| warning: false
#| message: false

### --------------------------------------------------------------------------###
## EuropeElects ----------------------------------------------------------------
# Read in data (REF: https://storage.googleapis.com/asapop-website-20220812/csv.html; https://filipvanlaenen.github.io/eopaod/pl.csv)
ee <- data.table::fread(
  # file = here::here("data_in", "pl.csv")
  input = "https://storage.googleapis.com/asapop-website-20220812/_csv/de.csv"
) |>
  janitor::clean_names()
# str(ee)

### --------------------------------------------------------------------------###
## Process the data ------------------------------------------------------------
# Reshape to long
ee_long <- ee |>
  tidyr::pivot_longer(
    cols = cdu_csu:other,
    # cols = c(which(names(ee) == "cdu_csu") : which(names(ee) == "other") ),
    names_to = "party_ee", values_to = "share"
  ) |>
  # recode date and share
  dplyr::mutate(
    fieldwork_start = as.Date(fieldwork_start),
    share = readr::parse_number(share)
  )

### --------------------------------------------------------------------------###
## Plot ------------------------------------------------------------------------
ee_long |>
  dplyr::filter(
    !is.na(party_ee) & !is.na(share) &
      party_ee %in% c(
        "af_d", "cdu", "cdu_csu", "cdu", "fdp", "fw", "grune",
        "link", "spd"
      ) &
      fieldwork_start >= as.Date("2024-01-01")
  ) |>
  ggplot(aes(x = fieldwork_start, y = share)) +
  geom_point(alpha = 0.2) +
  geom_smooth(aes(group = party_ee, colour = party_ee, fill = party_ee),
              method = "loess", alpha = 0.2
  ) +
  labs(
    colour = "", fill = "", x = "",
    title = "Parties' trends in the polls",
    caption = paste0("Source: EuropeElects. Data extracted on ", Sys.Date())
  ) +
  theme(
    plot.title.position = "plot",
    plot.title = element_text(face = "bold"),
    legend.position = "top"
  )
```

```{r}
#| include: false

#------------------------------------------------------------------------------#
rm(ee, ee_long); gc()
```

