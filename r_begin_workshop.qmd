---
title: "Data Analysis and Visualisation with R"
format:
  html:
    theme: united
    embed-resources: true
    code-fold: true
    code-summary: "Show the code"
    toc: true
    toc-depth: 2
    number-sections: true
    number-depth: 3
editor_options: 
  chunk_output_type: console
---

## Learning objectives

* Get familiar with an IDE (Integrated development environment).
* Know how to set up a data research project.
* Visualise data: choose an appropriate visualisation for your data; understand `ggplot2`'s layered approach.
* Be able to read in, process, and store data.
* A few advanced topics: functions; conditional execution, joins, loops.

In this doc, all code chunks are folded.
Readers are encouraged to immediately look at the code for Sections 5 and 6.
From Sections 7 onwards, the reader can start to jot down some initial code before seeing the solution.

Let's manage expectations:

* I tailored this workshop for a 2 hours session.
Depending on the overall level, we might not cover all the content - *that's ok*, you still get this [document](https://github.com/scipima/r_begin_workshop) if you want to explore further. 
* I won't be able to enter into any detail about `R` itself, statistics, or visualisation - and *that's also ok*. 
* The main objective here really is to give you a flavour of a few of the things that can be currently achieved with the kinds of software and hardware we have at our disposal.

I *assume* that you are now staring at an IDE - be it RStudio or VS Code - as per the email that was shared with you. 
Please shout if that is not the case!


## Intro
For this workshop, we borrow heavily from [Data Visualization for Social Science](https://socviz.co/index.html#preface).
This very markdown is based off the `R` package accompanying the book. 
You can use it to take notes, write your code, and produce a good-looking, reproducible document that records the work you have done. 

At the very top of the file is a section of *metadata*, or information about what the file is and what it does.
The metadata is delimited by three dashes at the start and another three at the end.
You can/should change the title, author, and date to the values that suit you.
Keep the `output` line as it is for now, however.
Each line in the metadata has a structure.
First the *key* (`title`, `author`, etc.), then a colon, and then the *value* associated with the key.
It is very picky when it comes with indentations, characters used, etc..


### This is a Quarto File
[Markdown](http://rmarkdown.rstudio.com) is a simple formatting syntax for authoring HTML, PDF, and MS Word documents, and is also the basis for [Quarto](https://quarto.org/). 

When you click the **Render** button a document will be generated that includes both content as well as the output of any embedded `R` code chunks within the document. 
A *code chunk* is a specially delimited section of the file. 
You can add one by moving the cursor to a blank line choosing `Code` > `Insert Chunk` from the RStudio menu. 
When you do, an empty chunk will appear:
```{r}
```

Code chunks are delimited by three backticks (found to the left of the 1 key on US and UK keyboards) at the start and end.
The opening backticks also have a pair of braces and the letter `r`, to indicate what language the chunk is written in (this is a clue to the fact that you can have multiple programming language in the document, such as `Python`, or `Bash`, or `SQL`).
You write your code inside the code chunks.
Write your notes and other material around them, as here.


## Know your editor
Please have a look at your screen.
Some of the most important things to learn now:

* Console
* Editor
* Environment

For this workshop, we rely on RStudio.
An alternative you may explore is [VS Code](https://code.visualstudio.com/).


## Before you begin to explore and analyse, set up your workspace
A number of functions and packages come already installed in what is generally referred to as `R base` (and you can actually see what's in it by typing `base::` in your console, and let the auto-complete suggest you the full list of functions ... ).
However, most of the things we'll do in this workshop need further `libraries`.
To install them, make sure you have an Internet connection. 
Then *manually* run the code in the chunk below. 
If you just render the document, this will be skipped - **however, if this is the first time you run this, make sure to manually run the code chunk below**. 
We do this because you only need to install these packages once, not every time you run this file. 
Either knit the chunk using the little green "play" arrow to the right of the chunk area, or copy and paste the text into the console window.
```{r}
#| eval: false
#| echo: false

# This code will not be evaluated automatically.
# Notice the eval = FALSE declaration in the options section of the code chunk

my_packages <- c(
    "bit", "devtools", "gapminder", "ggrepel", "here", "janitor", "scales",
    "tidyverse", "unvotes", "vroom"
)
install.packages(my_packages)
devtools::install_github("kjhealy/socviz")

# repo set-up -----------------------------------------------------------------#
# check if directory exists to dump processed files, figures, etc.
if ( !dir.exists( here::here("figures") ) ) {
    # if the directory does not exist, create it
    dir.create(here::here("figures"))
}
```


### Load Libraries
Once you installed your libraries, each time your run this document you **must** load them. 
If we do not load them, R will not be able to find the functions contained in these libraries. 
The `tidyverse` includes `ggplot2` (for data visualisation) and other libraries such as `dplyr` (for data manipulation).
We also load the `socviz` and `gapminder` libraries.
```{r}
#| include: false

## Load the libraries we will be using
library(tidyverse)
library(here)
library(gapminder)
library(ggrepel)
library(socviz)
```

Notice that here an option is set: `include=FALSE`. 
This tells `R` to run this code but not to include the output in the final document. 

When you click the **Render** button a document will be generated that includes both content as well as the output of any embedded `R` code chunks within the document.
You can embed an `R` code chunk like this:
```{r}
#| code-fold: show

gapminder::gapminder
```

A final note about the `here` [library](https://here.r-lib.org/).
That is a great help if you have properly set up your project as I just indicated. 
All paths will become relative, and you can forget about remembering where you are in your machine.
The essential thing is that it relies on the `.RProject` file that is created automatically for you when you create a `Project` with `RStudio`.


## Data Visualisation
### Few initial words
`R` has a consolidated role in data visualisation, well beyond the initial remit of statistics. 
A few examples:

* `R` at [FT](https://johnburnmurdoch.github.io/slides/r-ggplot/#/).
* `R` at [BBC](https://bbc.github.io/rcookbook/?ck_subscriber_id=2670369721&utm_source=convertkit&utm_medium=email&utm_campaign=What%E2%80%99s%20New%20in%20R:%20May%206,%202024%20-%2013817383).
* `R` in the *Best Practices for Data Visualisation* of the [Royal Statistical Society](https://royal-statistical-society.github.io/datavisguide/?ck_subscriber_id=2670369721&utm_source=convertkit&utm_medium=email&utm_campaign=What%E2%80%99s%20New%20in%20R:%20May%206,%202024%20-%2013817383). 
* Resources compiled by people from [Posit](https://themockup.blog/static/slides/intro-plot#1).


### Let's start with some actual code
Let's go back to the `gapminder` data we saw before. 
What kind of variables are we dealing with?
Let's remind ourselves.
```{r}
head(gapminder)
```

Technically, this data is in a *long* format, and you can immediately spot it by looking at the repeated values in the first 2 columns.
An example of a data in *wide* format is provided by [Eurostat](https://ec.europa.eu/eurostat/databrowser/view/sdg_08_10/default/table?lang=en).

We can explore the data in a more structured way, by checking what classes do we have as columns, and get some descriptive statistics.
```{r}
#| code-fold: show

str(gapminder) # classes and first values
glimpse(gapminder) # very similar, tidyverse way
summary(gapminder) # main stats
```

Notice how we use functions in `R`: we call a function - in the chunk above, `str`, `glimpse`, `summary` - and then include an argument between parentheses - in this case, the entire dataset.

Let's now start to analyse the data.
Say that we have an hypothesis regarding how life expectancy (`lifeExp`) changes in relations with GDP per capita (`gdpPercap`).
More precisely, we may think that as GDP per capita increases, so should life expectancy.
Graphically, if we look back at our data, we should see dots amassing in lower-left and upper-right sections of our graph. 
As these look like continuous variables, we could plot them in a [scatterplot](https://r-graph-gallery.com/).
There are at least two ways to create a plot within `ggplot`.
As you'll see, there are usually multiple ways to achieve the exact same output with these programming languages.

* We can **assign** a ggplot plot to an object called `p` (the name doesn't matter, we can call it `mike` if we want to), and then add (literally, using `+`) layers on top of it.
```{r}
#| code-fold: show

p <- ggplot(
    data = gapminder,
    mapping = aes(
        x = gdpPercap,
        y = lifeExp
    )
)
p + geom_point() # and you can keep on adding to this, see below ...
```

* Another way of achieving the same thing is as follows, which is more in line with a `tidyverse` style which heavily rely on pipes (`|>`). 
```{r}
#| eval: false
#| code-fold: show

gapminder |> # pipe
    ggplot(aes(x = gdpPercap, y = lifeExp)) + # layered ggplot approach
    geom_point()
```

Let's unpack what happens in the code above:

* we grab the data by calling gapminder
* on this data, we apply a function, namely ggplot
* calling a function on that data means we get the exact same output as the `p` object above.
Thus, we can add a layer on top of that ggplot object, that is the `geom_point`.

Let's go back to the graph now.
Sometimes, when we face a cloud like the one above, it is useful to plot a trendline on top, to understand the overall pattern. 
```{r}
# geom_smooth() using method = 'gam' and formula 'y ~ s(x, bs = "cs")' - GAM stands for generalised additive model
p + geom_point() +
    geom_smooth()
```

Perhaps that is too wiggly.
Using `method='lm'` (linear model) as an argument to `geom_smooth()` includes a simple OLS regression.
This also should make you aware of the danger of extrapolating from predictions of inappropriate models.
```{r}
# another way to get the same result
ggplot(
    data = gapminder,
    mapping = aes(
        x = gdpPercap,
        y = lifeExp
    )
) +
    geom_point() +
    geom_smooth(method = "lm")
```

Data is quite bunched up against the left side. 
Gross Domestic Product per capita is not normally distributed across our country years. 
The x-axis scale would probably look better if it were transformed from a linear scale to a log scale. 
For this we can use a function called `scale_x_log10()`. 
As you might expect this function scales the x-axis of a plot to a log 10 basis. 
To use it we just add it to the plot:
```{r}
p <- ggplot(
    data = gapminder,
    mapping = aes(
        x = gdpPercap,
        y = lifeExp
    )
)
p + geom_point() +
    geom_smooth(method = "gam") +
    scale_x_log10()
```

The labels on the tick-marks can be controlled through the `scale_*` functions.
Please notice that this we are using another library to have our tick labels in the dollar format. 
```{r}
p + geom_point() +
    geom_smooth(method = "gam") +
    scale_x_log10(labels = scales::dollar)
```

The viz above already is way more informative than the ones before, but there is more we can extract from the underlying data. 
For instance, now it may make sense to re-introduce the `lm` method we used before. 

In passing, also notice the use of different parameters in the viz. 
The `se` option commands the standard error, and we can switch it off by imputing `se=FALSE`. 
The `alpha` regulates the transparency of the objects, from 0 to 1. 
Further, we can add info and make more it *publication ready*.
```{r}
#| code-fold: show

ggplot(
    data = gapminder,
    mapping = aes(
        x = gdpPercap,
        y = lifeExp
    )
) +
    geom_point(alpha = 0.3) +
    geom_smooth(color = "orange", se = FALSE, linewidth = 1, method = "lm") +
    scale_x_log10(labels = scales::dollar) +
    labs(
        x = "GDP per capita (in log scale)",
        y = "Life Expectancy in Years",
        title = "Economic Growth and Life Expectancy",
        subtitle = "Data points are country-years",
        caption = "Source: Gapminder."
    ) +
    theme_minimal()
```

Another way to think about the underlying data is to remind ourselves about the groups we have, represented by the `continent` variable.
In the code below, we include colours as an aesthetic, and map it on continents.
Notice that, by including also a `colour` parameter in the `geom_smooth` function, we are overriding the `colour` mapping in the `ggplot` function.
This has important consequences, and is a further example of the **sequential** execution typical of many programming languages (left-right, top-bottom). 
```{r}
ggplot(
    data = gapminder,
    mapping = aes(
        x = gdpPercap,
        y = lifeExp,
        color = continent
    )
) +
    geom_point(alpha = 0.3) +
    geom_smooth(color = "orange", se = FALSE, linewidth = 1, method = "lm") +
    scale_x_log10(labels = scales::dollar)
```

Indeed, see what happens when we do not override it.
We can immediately appreciate that the slope of our regression is radically different depending on the groups - consider this your introduction to the *dangers of pooling*. 
```{r}
ggplot(
    data = gapminder,
    mapping = aes(
        x = gdpPercap,
        y = lifeExp,
        color = continent
    )
) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = "lm") +
    scale_x_log10(labels = scales::dollar)
```

However, this has become too cluttered, and it's hard to understand what's going on with all these colours and lines.
We reached saturation and need to revert to a simpler plot. 
```{r}
#| code-fold: show

ggplot(
    data = gapminder,
    mapping = aes(
        x = gdpPercap,
        y = lifeExp
    )
) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = "lm", colour = "orange", fill = "grey90") +
    facet_wrap(~continent) +
    scale_x_log10(labels = scales::dollar) +
    theme_minimal() +
    labs(
        x = "GDP per capita",
        y = "Life Expectancy in Years",
        title = "Economic Growth and Life Expectancy",
        subtitle = "Data points are country-years",
        caption = "Source: Gapminder."
    )
```

What you observe above is variously termed as a *small multiple* visualisation, or *faceted* visualisation. 
The key point is that we break down the data in meaningful groups, and analyse them separately.
This enables us to see more clearly similarities and differences.
Remember, all analyses are comparisons. 

A radically different take is to exploit the **time dimension**.
```{r}
#| code-fold: show

gapminder |>
    # subset the data to just the rows with these continents
    filter(continent %in% c("Asia", "Africa", "Europe", "Americas")) |>
    ggplot(aes(x = year, y = gdpPercap)) +
    geom_line(color = "gray80", aes(group = country)) +
    geom_smooth(linewidth = 1, method = "loess", se = FALSE) +
    scale_y_log10(labels = scales::dollar) +
    facet_wrap(~continent, ncol = 2) +
    theme_minimal() +
    labs(
        x = "Year",
        y = "GDP per capita",
        title = "GDP per capita on five continents (1952-2007)"
    )
```

A few takeaways from this.

* First, our visualisation obviously brings the reader's attention to the blue trend line by continent.
For instace, the value at the end of the time series for the grouped trend line in Asia is roughly where the start of the grouped time series is in Europe.
* Second, pay attention to the larger variation surrounding the trend line in continents such as Asia and Africa compared to the relatively tighter distribution in Europe.
Inter alia, this would have substantial consequences were we to fit a model to such data: the larger variance in the former continents would mean larger uncertainty in our estimates compared to the European case.
In another case that you may be familiar with, this could happen to parties' polls, where we could have substantial variation in the polls translating into larger margin of errors in projections.
```{r}
rm(p)
```


## UN Votes
Also in this Section the data comes directly from the library, exactly as in the case of `gapminder`.
How many tables are we dealing with?
```{r}
library(unvotes)
un_votes <- unvotes::un_votes
un_roll_calls <- unvotes::un_roll_calls
```


### Counting categorical variables
If we want to get a quick glimpse of the distribution of votes, we could just simply count how many there are by unique value of the `vote` variable.

::: {.panel-tabset}

#### R tidyverse

```{r}
#| code-fold: show

un_votes |>
    count(vote, sort = TRUE)
```

#### Base R

```{r}
#| code-fold: show

with(
    un_votes,
    tapply(X = vote, INDEX = vote, FUN = length)
)
```

#### R data.table

```{r}
#| code-fold: show

data.table::setDT(un_votes)[, .N, by = list(vote)]
```

:::


Say that we don't particularly like this representation of the data, and we would like to recode the vote variable.
We could achieve that like so. 
```{r}
# recode
un_votes$vote_int[un_votes$vote == "yes"] <- 1L
un_votes$vote_int[un_votes$vote == "no"] <- -1L
un_votes$vote_int[un_votes$vote == "abstain"] <- 0L
# check
table(un_votes$vote_int, un_votes$vote, exclude = NULL)
```


### Aggregations
Share of votes, by country.
Check how you can do it in 2 ways, with a rough appreciation of the timing difference.
I'll walk you through just the `tidyverse` implementation.

* we call the data
* we group by a variable of interest
* we squash all the data by the grouping variable, and calculate two new variables, namely the total votes, and the number of yes.
* we close the grouped calculations by calling off the `group_by` - that is, `ungroup()`
* we create a new variable, which is just the share


::: {.panel-tabset}

#### R tidyverse

```{r}
#| code-fold: show

library(tictoc) # library for timing

# aggregation: tidyverse style ------------------------------------------------#
tictoc::tic() # timing on

un_votes |>
    group_by(country) |> # group by country
    summarise(
        n_votes = n(), # count votes
        n_yes = sum(vote == "yes", na.rm = TRUE) # count yes
    ) |>
    ungroup() |> # ungroup operations
    mutate(pct_yes = n_yes / n_votes) # create new col, no summarise

tictoc::toc() # timing off
```

#### R data.table

```{r}
#| code-fold: show

# aggregation: data.table style -----------------------------------------------#
tictoc::tic()

data.table::setDT(un_votes)[
    , list(
        n_votes = .N,
        n_yes = sum(vote == "yes", na.rm = TRUE)
    ),
    keyby = list(country)
][
    , pct_yes := n_yes / n_votes
][]

tictoc::toc()
```

:::


### Functions
As a more *advanced topic*, if you know you'll execute that same function over and over, you can re-package that in a `function`.
Notice a few things:

* name/position of arguments
* default values
* calling functions explicitly from `namespace`

```{r}
#| code-fold: show

# write your first function
summarise_votes <- function(data_in = un_votes, min_tot_votes = 10L) {
    data_in |>
        dplyr::summarise(
            n_votes = n(),
            n_yes = sum(vote == "yes", na.rm = TRUE)
        ) |>
        dplyr::filter(n_votes >= min_tot_votes) |> # filter by at least this many votes
        dplyr::mutate(pct_yes = n_yes / n_votes) |>
        dplyr::arrange(dplyr::desc(pct_yes)) # arrange by pct_yes
}

# test, we should get the same answer as above ... -------------------------------#
un_votes |>
    group_by(country) |>
    summarise_votes() |> 
    ungroup()
```


### Trends over time in `yes` votes
Similar to the previous case - the `gapminder` data - we may be interested in the evolution over time. 
In order to get that piece of info, we need to introduce another topic, namely **merges** (or **joins**).
Indeed, votes' dates are recorded in the other dataset, `un_roll_calls`.
Notice the use of `left_join`.
```{r}
#| code-fold: show

un_votes <- un_votes |> # start with this data
    dplyr::left_join( # join only the matches in the origin data
        y = un_roll_calls |> # from this data, we keep only the matches in the incoming data from the pipe
            dplyr::select(rcid, date), # we also subset the right-hand side data on the fly
        by = "rcid" # we merge by this col
    ) |>
    dplyr::mutate(year = lubridate::year(date)) # create a year col

# same result, this time in base R
# merge(un_votes, un_roll_calls,
#       by = "rcid",
#       all.x = TRUE, all.y = TRUE) # left join
```

Graph the evolution over time of share of yes.
```{r}
un_votes |>
    dplyr::group_by(year) |>
    summarise_votes() |>
    ungroup() |>
    ggplot(aes(x = year, y = pct_yes)) +
    geom_line() +
    expand_limits(y = 0) +
    theme_minimal()
```

Again, beware of pooling.
What if we split the overall trend by country?
```{r}
un_votes |>
    dplyr::filter(country %in% c("France", "Germany", "Sweden", "United States")) |>
    dplyr::group_by(country, year) |> # notice we're grouping by 2 cols here
    summarise_votes() |>
    ungroup() |>
    ggplot(aes(x = year, y = pct_yes, group = country)) +
    geom_line(aes(colour = country)) +
    scale_y_continuous(labels = scales::label_percent()) +
    expand_limits(y = 0) +
    theme_minimal() +
    theme(legend.position = "top")
```

This can become too cluttered very soon.
How do we deal with that?
Faceting is your friend.
However, depending on the country, the plot may be similar or way off from the aggregate.
So, we can remind ourselves in every graph what the overall trend is.
Now, you can tweak your code to represent your overall trend in any way you like (e.g. regional data?). 
```{r}
un_votes |>
    dplyr::filter(
        country %in% c(
            "France", "Germany", "Sweden", "United States", "Russia", "China"
        )
    ) |>
    dplyr::group_by(country, year) |> # notice we're grouping by 2 cols here
    summarise_votes() |>
    ungroup() |>
    ggplot() + # notice this is empty
    geom_line(aes(
        x = year, y = pct_yes, # we're passing aes directly to geoms
        group = country, colour = country
    )) +
    # here we're providing new data, aggregated on the fly
    geom_line(
        data = un_votes |>
            dplyr::group_by(year) |>
            summarise_votes() |>
            ungroup(),
        aes(x = year, y = pct_yes),
        colour = "grey", linewidth = 1
    ) +
    facet_wrap(~country, ncol = 2) +
    scale_y_continuous(labels = scales::label_percent()) +
    labs(colour = "", x = "Year", y = "Share of yes (%)") +
    expand_limits(y = 0) +
    theme_minimal() +
    theme(legend.position = "top")
ggsave(here::here("figures", "unvotes_yesbycountry.png"), 
    dpi = 300, height = 6, width = 8, device = "png", bg = "white")
```

```{r}
rm(un_roll_calls, un_votes, summarise_votes)
gc()
```



## EP Data
### How many documents in each Committee?
Say that you want to organise the Group so that the Committees that receive the most files are also allocated more manpower.
How would you go about answering this question?

* Find the data.
* Load it in your machine.
* Process it.
* Spit back the result.

```{r}
#| code-fold: show
#| warning: false
#| message: false

#------------------------------------------------------------------------------#
# Download and Process EP Open Data on Plenary Docs ----------------------------
#------------------------------------------------------------------------------#

#' Collects .csv files from the [EP Open Data Portal](https://data.europarl.europa.eu/en/datasets?language=en&order=RELEVANCE&dataThemeFamily=dataset.theme.EP_PLEN_DOC).
#' The code then proceeds to tidy such data and write to disk.

## Texts tabled: read csv & append it all together -----------------------------
docs_csv <- c(
    "https://data.europarl.europa.eu/distribution/plenary-documents_2024_15_en.csv",
    "https://data.europarl.europa.eu/distribution/plenary-documents_2023_52_en.csv",
    "https://data.europarl.europa.eu/distribution/plenary-documents_2022_33_en.csv",
    "https://data.europarl.europa.eu/distribution/plenary-documents_2021_22_en.csv",
    "https://data.europarl.europa.eu/distribution/plenary-documents_2020_14_en.csv",
    "https://data.europarl.europa.eu/distribution/plenary-documents_2019_5_en.csv"
)
# read all .csv at once
docs_list <- lapply(X = docs_csv, readr::read_csv)
# append all .csv together ----------------------------------------------------#
plenary_docs <- data.table::rbindlist(l = docs_list, use.names = T, fill = T)

#------------------------------------------------------------------------------#
# Same thing, as a loop -------------------------------------------------------#
# docs_list <- NULL
# for (i_doc in docs_csv) {
#   print(i_doc)
#   docs_list[[i_doc]] <- read.csv(file = i_doc) }
#------------------------------------------------------------------------------#

rm(docs_list, docs_csv)
gc()
```

Extract the year feature, similar to what we did above, so that we can group the data afterwards.  
```{r}
# extract year
plenary_docs$year <- lubridate::year(plenary_docs$document_date)
# unique(plenary_docs$year) # check
```

Do we have duplicate entries in the doc id?
We can table the number of times each document appears in the file.
```{r}
table(plenary_docs$document_identifier, exclude = NULL) |>
    head(5)
```

We can then take the mean of the cross-tabulation above.
```{r}
mean(table(plenary_docs$document_identifier, exclude = NULL))
```

We can then extract the unique names of the Committees in the text.
```{r}
head(sort(unique(plenary_docs$document_creator_organization)))
tail(sort(unique(plenary_docs$document_creator_organization)))
```

There are way too many values here.
We need to filter on the string to get just the pattern of interest, namely `Committee on `.
```{r}
#| code-fold: show

plenary_docs |>
    dplyr::filter(
        grepl(
            pattern = "Committee on ",
            x = document_creator_organization
        )
    ) |>
    dplyr::group_by(document_creator_organization) |>
    dplyr::summarise(count = n()) |>
    dplyr::arrange(dplyr::desc(count))
```

However, we're not done here.
If we actually check the tail of the table above - by simply running `tail()` -, we can notice something weird. 
```{r}
#| code-fold: show

plenary_docs |>
    dplyr::filter(
        grepl(
            pattern = "Committee on ",
            x = document_creator_organization
        )
    ) |>
    tidyr::separate_longer_delim(cols = document_creator_organization, delim = ";") |>
    dplyr::mutate(
        document_creator_organization = trimws(document_creator_organization)
    ) |>
    dplyr::select(document_identifier, document_creator_organization) %>%
    dplyr::group_by(document_identifier) |>
    dplyr::summarise(committee_count = n()) |>
    dplyr::ungroup() |>
    dplyr::arrange(dplyr::desc(committee_count))
```

We need to split the cells that contain multiple Committees.
```{r}
#| code-fold: show

comm_doc_count <- plenary_docs |>
    dplyr::filter(
        grepl(
            pattern = "Committee on ",
            x = document_creator_organization
        )
    ) |>
    tidyr::separate_longer_delim(cols = document_creator_organization, delim = ";") |>
    dplyr::mutate(
        document_creator_organization = trimws(document_creator_organization)
    ) |>
    dplyr::group_by(document_creator_organization) |>
    dplyr::summarise(doc_count = n()) |>
    dplyr::ungroup() |>
    dplyr::arrange(dplyr::desc(doc_count))
```

And we can plot to visually inspect that everything went as intended.
```{r}
comm_doc_count |>
    dplyr::filter(doc_count > 1L) |>
    dplyr::mutate(document_creator_organization = forcats::fct_reorder(
        .f = document_creator_organization, .x = doc_count, .fun = max
    )) |>
    ggplot(aes(x = document_creator_organization, y = doc_count)) +
    geom_col(colour = "black", fill = "grey", linewidth = 0.1) +
    coord_flip() +
    labs(x = "", y = "Count") +
    theme_minimal()
```

Now, try to come up with your solution: what if you wanted also to know the volumes per Committee per year?
```{r}
#| fig-width: 8
#| fig-height: 10
#| warning: false
#| message: false

plenary_docs |>
    dplyr::filter(grepl(
        pattern = "Committee on ",
        x = document_creator_organization
    )) |>
    tidyr::separate_longer_delim(cols = document_creator_organization, delim = ";") |>
    dplyr::mutate(
        document_creator_organization = trimws(
            gsub(
                pattern = "Committee on |Committee on the ", replacement = "",
                x = document_creator_organization
            )
        )
    ) |>
    dplyr::group_by(document_creator_organization, year) |>
    dplyr::summarise(doc_count = n()) |>
    dplyr::ungroup() |>
    dplyr::arrange(dplyr::desc(doc_count)) |>
    dplyr::filter(doc_count > 1L) |>
    ggplot(aes(x = year, y = doc_count)) +
    geom_line(colour = "black", linewidth = 1) +
    facet_wrap(~document_creator_organization,
        ncol = 4,
        labeller = labeller(document_creator_organization = label_wrap_gen(20))
    ) +
    labs(x = "", y = "Count") +
    theme_minimal()
```

```{r}
#| include: false
rm(plenary_docs); gc()
```


## Polls (EE, de, means, aggregations)
This section is the most advanced.
Polls are extracted from [EuropeElects](https://europeelects.eu/germany/).
```{r}
#| fig-width: 8
#| warning: false
#| message: false

### --------------------------------------------------------------------------###
## EuropeElects ----------------------------------------------------------------
# Read in data (REF: https://storage.googleapis.com/asapop-website-20220812/csv.html; https://filipvanlaenen.github.io/eopaod/pl.csv)
ee <- data.table::fread(
    # file = here::here("data_in", "pl.csv")
    input = "https://storage.googleapis.com/asapop-website-20220812/_csv/de.csv"
) |>
    janitor::clean_names()
# str(ee)

### --------------------------------------------------------------------------###
## Process the data ------------------------------------------------------------
# Reshape to long
ee_long <- ee |>
    tidyr::pivot_longer(
        cols = cdu_csu:other,
        # cols = c(which(names(ee) == "cdu_csu") : which(names(ee) == "other") ),
        names_to = "party_ee", values_to = "share"
    ) |>
    # recode date and share
    dplyr::mutate(
        fieldwork_start = as.Date(fieldwork_start),
        share = readr::parse_number(share)
    )

### --------------------------------------------------------------------------###
## Plot ------------------------------------------------------------------------
ee_long |>
    dplyr::filter(
        !is.na(party_ee) & !is.na(share) &
            party_ee %in% c(
                "af_d", "cdu", "cdu_csu", "cdu", "fdp", "fw", "grune",
                "link", "spd"
            ) &
            fieldwork_start >= as.Date("2024-01-01")
    ) |>
    ggplot(aes(x = fieldwork_start, y = share)) +
    geom_point(alpha = 0.2) +
    geom_smooth(aes(group = party_ee, colour = party_ee, fill = party_ee),
        method = "loess", alpha = 0.2
    ) +
    labs(
        colour = "", fill = "", x = "",
        title = "Parties' trends in the polls",
        caption = paste0("Source: EuropeElects. Data extracted on ", Sys.Date())
    ) +
    theme_minimal() +
    theme(
        plot.title.position = "plot",
        plot.title = element_text(face = "bold"),
        legend.position = "top"
    )
```
